---
title: "Cross-Validation"
subtitle: "<br><br> Psych 101x"
author: "Amanda D. Perez, PhD"
output:
  xaringan::moon_reader:
    css: ["https://amandadperez.github.io/TutoRials/docs/Personal/BASIS/xaringan-themer.css", "https://amandadperez.github.io/TutoRials/docs/Personal/BASIS/slides.css"]
    lib_dir: libs
    nature:
      ratio: "16:9"
      highlightLines: true
      countIncrementalSlides: false
---


```{r child = "setup.Rmd"}
```


```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(emo)
xaringanExtra::use_scribble()
# highlightStyle: solarized-dark

library(tidymodels)
library(ggtext)
library(knitr)
library(kableExtra)
options(dplyr.print_min = 10, dplyr.print_max = 6)

library(gghighlight)
library(knitr)
library(caret)
set.seed(1234)
```

class: middle

# Data and exploration

---

background-image: url("img/the-office.jpeg")
class: middle

---

## Data

```{r}
office_ratings <- read_csv("data/office_ratings.csv")
office_ratings
```

.footnote[
.small[
Source: The data come from [data.world](https://data.world/anujjain7/the-office-imdb-ratings-dataset), by way of [TidyTuesday](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-03-17/readme.md). 
]
]

---

## IMDB ratings

.panelset[
```{r panelset = c(output = "Plot", source = "Code")}
ggplot(office_ratings, aes(x = imdb_rating)) +
  geom_histogram(binwidth = 0.25) +
  labs(
    title = "The Office ratings",
    x = "IMDB Rating"
  )
```
]

---

## IMDB ratings vs. number of votes

.panelset[
```{r panelset = c(output = "Plot", source = "Code"), out.width="55%"}
ggplot(office_ratings, aes(x = total_votes, y = imdb_rating, color = season)) +
  geom_jitter(alpha = 0.7) +
  labs(
    title = "The Office ratings",
    x = "Total votes",
    y = "IMDB Rating",
    color = "Season"
  )
```
]

---

## Outliers

.panelset[
```{r panelset = c(output = "Plot", source = "Code"), out.width="55%"}
ggplot(office_ratings, aes(x = total_votes, y = imdb_rating)) +
  geom_jitter() +
  gghighlight(total_votes > 4000, label_key = title) +
  labs(
    title = "The Office ratings",
    x = "Total votes",
    y = "IMDB Rating"
  )
```
]

.footnote[
.small[
If you like the [Dinner Party](https://www.imdb.com/title/tt1031477/) episode, I highly recommend this ["oral history"](https://www.rollingstone.com/tv/tv-features/that-one-night-the-oral-history-of-the-greatest-office-episode-ever-629472/) of the episode published on Rolling Stone magazine.
]
]

---

## IMDB ratings vs. seasons

.panelset[
```{r panelset = c(output = "Plot", source = "Code"), out.width="55%"}
ggplot(office_ratings, aes(x = factor(season), y = imdb_rating, color = season)) +
  geom_boxplot() +
  geom_jitter() +
  guides(color = "none") +
  labs(
    title = "The Office ratings",
    x = "Season",
    y = "IMDB Rating"
  )
```
]

---

class: middle

# Modeling

---

## Train / test

- Create an initial split index

```{r}
set.seed(1234) #set the seed so we can all reproduce results
office_split <- createDataPartition(office_ratings$imdb_rating, p = .8, list = FALSE, times = 1)
```

--
.pull-left[
- Save training data
```{r}
office_train <- office_ratings[office_split, ]
dim(office_train)
```
]

--
.pull-right[
- Save testing data
```{r}
office_test  <- office_ratings[-office_split, ]
dim(office_test)
```
]


---

## Fit model with training data

.panelset[
.panel[.panel-name[Code]
```{r}
office_fit <- lm(imdb_rating ~ season + total_votes, data = office_train)
```
]
.panel[.panel-name[Output]
.small[
```{r}
tidy(office_fit) 
```
]
]
]

---

class: middle

# Evaluate model

---

## Make predictions for training data

```{r, warning = F, message = F}
office_train_pred <- predict(office_fit, office_train) %>%
  bind_cols(office_train %>% select(imdb_rating, title))
names(office_train_pred)[1] <- "Prediction"
office_train_pred
```

---

## R-squared

Percentage of variability in the IMDB ratings explained by the model

```{r}
R2(office_train_pred$Prediction, office_train$imdb_rating)
```

--

.question[
Are models with high or low $R^2$ more preferable?
]

---

## RMSE

An alternative model performance statistic: **root mean square error**

$$ RMSE = \sqrt{\frac{\sum_{i = 1}^n (y_i - \hat{y}_i)^2}{n}} $$

--

```{r rmse-train}

RMSE(office_train_pred$Prediction, office_train$imdb_rating)

```

--

.question[
Are models with high or low RMSE are more preferable?
]

---

## Interpreting RMSE

.question[
Is this RMSE considered low or high?
]

```{r ref.label="rmse-train"}
```

--

```{r}
office_train %>%
  summarise(min = min(imdb_rating), max = max(imdb_rating))
```

---

class: middle

.hand[
.light-blue[
but, really, who cares about predictions on .pink[training] data?
]
]

---

## Make predictions for testing data

```{r, warning F, message = F}
office_test_pred <- predict(office_fit, office_test) %>%
  bind_cols(office_test %>% select(imdb_rating, title))
names(office_test_pred)[1] <- "Prediction"
office_test_pred

```

---

## Evaluate performance on testing data

- RMSE of model fit to testing data

```{r}
RMSE(office_test_pred$Prediction, office_test$imdb_rating)
```

- $R^2$ of model fit to testing data

```{r}
R2(office_test_pred$Prediction, office_test$imdb_rating)
```

---

## Training vs. testing

<br>

```{r echo=FALSE}
rmse_train <- RMSE(office_train_pred$Prediction, office_train$imdb_rating) %>% 
  round(3)
rsq_train <- R2(office_train_pred$Prediction, office_train$imdb_rating) %>%
  round(3)
rmse_test <- RMSE(office_test_pred$Prediction, office_test$imdb_rating) %>%
  round(3)
rsq_test <- R2(office_test_pred$Prediction, office_test$imdb_rating) %>% 
  round(3)
tibble(
  data      = c(rep("train", 2), rep("test", 2)),
  estimate  = c(rmse_train, rsq_train, rmse_test, rsq_test),
  metric    = c("RMSE", "R-squared", "RMSE", "R-squared")
) %>%
  pivot_wider(names_from = data, values_from = estimate) %>%
  bind_cols(comparison = c("RMSE lower for training", "R-squared higher for training")) %>%
  kable()
```

---

## Evaluating performance on training data

-  The training set does not have the capacity to be a good arbiter of performance.

--
- It is not an independent piece of information; predicting the training set can only reflect what the model already knows.

--
- Suppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test.

.footnote[
.small[
Source: [tidymodels.org](https://www.tidymodels.org/start/resampling/)
]
]

---

class: middle

# Cross validation

---

## Cross validation

More specifically, **v-fold cross validation**:

- Shuffle your data v partitions
- Use 1 partition for validation, and the remaining v-1 partitions for training
- Repeat v times

.footnote[
.small[
You might also heard of this referred to as k-fold cross validation.
]
]

---

## Cross validation

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics("img/cross-validation.png")
```

---

## Set out Cross-Validation Specifications

.pull-left[
```{r}
set.seed(345)
ctrl_specs <- trainControl(method = "cv", number = 5, 
                           savePredictions = "all")
```
]
.pull-right[
```{r echo=FALSE, out.width="100%", fig.align="right"}
knitr::include_graphics("img/cross-validation.png")
```
]

---

## Fit resamples

.pull-left[
```{r}
set.seed(456)
office_fit_rs <- train(imdb_rating ~ season + total_votes,
                 data = office_train, method = "lm", trControl = ctrl_specs)
```
]
.pull-right[
```{r echo=FALSE, out.width="100%", fig.align="right"}
knitr::include_graphics("img/cross-validation-animated.gif")
```
]

---

## Collect CV metrics

```{r}
print(office_fit_rs)
```

---

## Deeper look into CV metrics


```{r}
office_fit_rs$resample
```


---

## See output in terms of regression coefficients

```{r}
summary(office_fit_rs)
```

---

## Assess variable importance of the predictors

```{r, warning = F, message = F}
varImp(office_fit_rs)
```

---


## Apply our model to the office_test dataframe


```{r}
office_predictions <- predict(office_fit_rs, newdata = office_test) 
```

---

## Evaluate Model Metrics for Testing Data


```{r}
data.frame(R2 = R2(office_predictions, office_test$imdb_rating),
            RMSE = RMSE(office_predictions, office_test$imdb_rating))

```

---

## Compare our Training Metrics to our Testing Metrics


```{r}
data.frame(office_fit_rs$results[3],
            R2_testing = R2(office_predictions, office_test$imdb_rating),
            office_fit_rs$results[2],
            RMSE_testing = RMSE(office_predictions, office_test$imdb_rating))

```